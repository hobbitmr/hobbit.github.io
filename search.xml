<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[特征缩放(feature scaling)]]></title>
    <url>%2F2018%2F12%2F05%2F%E7%89%B9%E5%BE%81%E7%BC%A9%E6%94%BE%2F</url>
    <content type="text"><![CDATA[特征缩放是为了，消除不同特征直接，数据差异过大的问题。常用有两种形式：标准化和归一化。 标准化 $x_{stand} = { {x-mean(x)}\over StandardDevivation(x)}$ mean(x) 为均值 StandardDevivation(x) 为标准方差 得到值为均值为0，方差为1 归一化 $x_{norm} = { {x - min(x)}\over max(x)-min(x)} $ 将x缩放到[0，1]之间]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[inearRegresstion推导以及代码实现]]></title>
    <url>%2F2018%2F11%2F29%2F11%2F</url>
    <content type="text"><![CDATA[有一个场景是这样，根据已知数据集，去预测房价的走向，这个是属于回归问题，所以我们使用线性回归来解决这个问题。我们先来看看线性回归中最简单的形式。 1.线性回归公式 假设输入为$X={x_1,x_2,x_3...x_m}$ m个样本，为了简单，我们先假设每个样本只有一个特征。为则函数模型为 $z(x_i)= wx_i+b$ 我们采用最小乘二法作为损失函数： $l(z)= (z-y_i)^2$ 那么成本函数，就是所有样本的损失函数加起来，然后平均值。 $J = {1\over 2m}\sum_{i=1}^ml ={1\over 2m}\sum_{i=1}^m(z(x_i)-y_i)^2$ 那么可以看到，如果我们预测的值跟实际值越相近，那么$J$的值就越小，忧郁$J$是凸函数，所以我们需要找到$J$函数的最小值，如果当函数导数为0的时候，就是$J$函数的最小值，所以我们可以通过梯度下降法，求得最佳的w和b，取得最小的$J$函数的最小值. 2.单特征梯度下降 求出w和b的导数。然后最w和b参数进行优化， 找到$J函数$全局最小值或者局部最小值 对 $w$ 求导： ${dJ\over dw} = {1\over 2m}\sum_{i=1}^m {dl\over dz} * {dz\over dw} $ ${dJ\over dw} = {1\over 2m}\sum_{i=1}^m {2*(z(x_i)-y_i)} * x_i $ ${dJ\over dw} = {1\over m}\sum_{i=1}^m {(z(x_i)-y_i)} * x_i $ 对 $b$ 求导： ${dJ\over db} = {1\over 2m}\sum_{i=1}^m {dj\over dz} * {dz\over db} $ ${dJ\over db} = {1\over 2m}\sum_{i=1}^m {2 * (z(x_i)-y_i)}$ ${dJ\over db} = {1\over m}\sum_{i=1}^m {(z(x_i)-y_i)}$ 然后 $w = w-{dJ\over dw}$ $b = b-{dJ\over db}$ 2.多特征梯度下降 这个是假设x只有一个特征，但是实际上数据肯定是多维度的，有多个属性。所以我们对上面的公式做个简单的修改。 假设 $x_i$的维度m，那么$x_i={x_i^1,x_i^2,x_i^3,...,x_i^n}$的列向量 假设 $W={w^1,w^2,w^3,...,w^n}$的列向量 那么由此得到: $z(x_i)= w^1x_i^1+w^2x_i^2+w^3x_i^3+...+w^nx_i^n +b = W^Tx_i+b$ 当$x_i$为多个维度之后，除了$z$这个函数跟之前不太一样，其他的基本都是一样的. 所以我们对$W$和$b$求导 对 $W$ 求导 ${dJ\over dW} = {1\over 2m}\sum_{i=1}^m {dl\over dz} * {dz\over dW} $ ${dJ\over dW} = {1\over 2m}\sum_{i=1}^m {2*(z(x_i)-y_i)} * x_i $ ${dJ\over dW} = {1\over m}\sum_{i=1}^m {(z(x_i)-y_i)} * x_i $ 这里的$dW$最后的值是一个(n,1)的列向量，因为$x_i$是(n,1)的列向量 而db是没有变。 我们对数据集进行 3.批量梯度下降: 上面我们所推导的公式，都是基于一个样本来进行推导,下面我们进行多个样本的梯度下降推导。 3.代码实现:]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
</search>
