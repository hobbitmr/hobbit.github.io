<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[导数公式]]></title>
    <url>%2F2018%2F12%2F06%2F%E5%AF%BC%E6%95%B0%E5%85%AC%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[基本初等函数求导公式 $(C)' = 0$ $(x^u)'=ux^{u-1}$ $(\sin x)' = \cos x $ $(\tan x)' = \sec^2 x $ $(\cos x)' = -\sin x $ $(\cot x)' = - \csc^2 x$ $(\sec x)' =\sec x * \tan x$ $(\csc x)' = -\csc x * \cot x$ $(a^x)' = a^x \ln a$ $(e^x)' = e^x$ $(\log_a x)' = {1 \over x\ln a} $ $(\ln x)' = {1 \over x}$ $(\arcsin x)' = {\frac1{\sqrt{1-x^2}} }$ $(\arccos x)' = {-\frac1{\sqrt{1-x^2}} }$ $(\arctan x)' = {\frac1{1+x^2} }$ $(\arctan x)' = {-\frac1{1+x^2} }$ 函数求导法则 设 $z =z(x),f = f(x)$ 都可导，则： 1.$(u{\pm}f)'=u'\pm v'$ 2.$(uv)'=u'v + uv'$ 3.$(\frac{u}{v})' = \frac {u'v-uv'}{v^2}$ 链式法则 设 $z =z(x),f = f(z)$ 都可导，则: $\frac{\partial f}{\partial x} = \frac{\partial f}{\partial z} * \frac{\partial z}{\partial x}$ 矩阵转置求导(前导不变，后导转置) $(AX)' = A^T$ $(XA)' = A$ $(x^TA)' = A$ $(Ax^T)' = A^T$]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[特征缩放(feature scaling)]]></title>
    <url>%2F2018%2F12%2F05%2F%E7%89%B9%E5%BE%81%E7%BC%A9%E6%94%BE%2F</url>
    <content type="text"><![CDATA[特征缩放是为了，消除不同特征直接，数据差异过大的问题。 常用有两种形式：标准化和归一化。 标准化 $x_{stand} = { {x-mean(x)}\over StandardDevivation(x)}$ mean(x) 为均值 StandardDevivation(x) 为标准方差 得到值为均值为0，方差为1 归一化 $x_{norm} = { {x - min(x)}\over max(x)-min(x)} $ 将x缩放到[0，1]之间]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[LinearRegresstion推导以及代码实现]]></title>
    <url>%2F2018%2F11%2F29%2FLinearRegresstion%E6%8E%A8%E5%AF%BC%E4%BB%A5%E5%8F%8A%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[有一个场景是这样，根据已知数据集，去预测房价的走向，这个是属于回归问题，所以我们使用线性回归来解决这个问题。我们先来看看线性回归中最简单的形式。 1.线性回归公式 假设输入为$X={x_1,x_2,x_3...x_m}$ m个样本，为了简单，我们先假设每个样本只有一个特征。为则函数模型为 $z(x_i)= wx_i+b$ 我们采用最小乘二法作为损失函数： $l(z)= (z-y_i)^2$ 那么成本函数，就是所有样本的损失函数加起来，然后平均值。 $j = {1\over 2m}\sum_{i=1}^ml ={1\over 2m}\sum_{i=1}^m(z(x_i)-y_i)^2$ 那么可以看到，如果我们预测的值跟实际值越相近，那么$J$的值就越小，忧郁$J$是凸函数，所以我们需要找到$J$函数的最小值，如果当函数导数为0的时候，就是$J$函数的最小值，所以我们可以通过梯度下降法，求得最佳的w和b，取得最小的$J$函数的最小值. 2.单特征梯度下降 求出w和b的导数。然后最w和b参数进行优化， 找到$J函数$全局最小值或者局部最小值 对 $w$ 求导: ${dj\over dw} = {1\over 2m}\sum_{i=1}^m {dl\over dz} * {dz\over dw} $ ${dj\over dw} = {1\over 2m}\sum_{i=1}^m {2*(z(x_i)-y_i)} * x_i $ ${dj\over dw} = {1\over m}\sum_{i=1}^m {(z(x_i)-y_i)} * x_i $ 对 $b$ 求导： ${dj\over db} = {1\over 2m}\sum_{i=1}^m {dl\over dz} * {dz\over db} $ ${dj\over db} = {1\over 2m}\sum_{i=1}^m {2 * (z(x_i)-y_i)}$ ${dj\over db} = {1\over m}\sum_{i=1}^m {(z(x_i)-y_i)}$ 然后得到 $w = w-{dJ\over dw}$ $b = b-{dJ\over db}$ 2.多特征梯度下降 这个是假设x只有一个特征，但是实际上数据肯定是多维度的，有多个属性。所以我们对上面的公式做个简单的修改. 假设 $x_i$的维度m，那么$x_i={x_i^1,x_i^2,x_i^3,...,x_i^n}$的列向量 假设 $W={w^1,w^2,w^3,...,w^n}$的列向量 那么由此得到: $z(x_i)= w^1x_i^1+w^2x_i^2+w^3x_i^3+...+w^nx_i^n +b = W^Tx_i+b$ 当$x_i$为多个维度之后，除了$z$这个函数跟之前不太一样，其他的基本都是一样的.所以我们对$W$和$b$求导: 对 $W$ 求导: ${dJ\over dW} = {1\over 2m}\sum_{i=1}^m {dl\over dz} * {dz\over dW} $ ${dJ\over dW} = {1\over 2m}\sum_{i=1}^m {2*(z(x_i)-y_i)} * x_i $ ${dJ\over dW} = {1\over m}\sum_{i=1}^m {(z(x_i)-y_i)} * x_i $ 因为$x_i$是(n,1)的列向量,这里的$dW$最后的值是一个(n,1)的列向量. db没有变,因为对dz/db求导,实际上得到的值是1 3.批量梯度下降: 上面我们所推导的公式，都是基于一个样本来进行推导,下面我们进行多个样本的梯度下降推导。 我们对样本进行批量输入，不再一个一个的输入.我们假设X 是$x_i$的集合，所以由于$x_i$有n个特征，X就是一个(m,n)的矩阵，而Y就是一个(m,1)的矩阵,W为(n,1)的矩阵 我们之前只有一行的时候，一般式为 $z(x_i)= W^Tx_i+b$ ， 因为x_i和W都是列向量，所以把W放在前面，需要对W转置成行向量。 为了计算方便，我们这里吧X定义为 m行n列的向量。将$x_i$定义为行向量,而W还是列向量，所以就得到就把X放在前面. $Z(X)= XW+b$ 此时的Z为(m,1) 损失函数： $J(Z)= {1\over 2m}* (Z-Y)^T(Z-Y)$ 此时J为实数 对 $W$ 求导: ${dJ\over dW} = {1\over 2m}* {dJ\over dZ} * {dZ\over dW} $ ${dJ\over dZ} = (Z-Y)+((Z-Y)^T)^T=2(Z-Y)$ ${dJ\over dW} = {1\over 2m} * 2* X^T{(Z-Y)} $ ${dJ\over dW} = {1\over m} * X^T{(Z-Y)}$ 4.代码实现: 终于推导完全，接下来我们看看代码怎么实现吧 12345678910111213141516171819202122232425262728293031323334353637383940414243444546class LinearRegression: W =None b = None def __init__(self,leaning_rate=0.001): self.leaning_rate=leaning_rate def fit(self,input_x,input_y): n_sample=input_x.shape[0] if self.W is None: self.W = np.random.random((input_x.shape[1], 1)) if self.b is None: self.b = 0.1 # 对应的公式 Z=XW+b fz = np.matmul(input_x, self.W) + self.b loss_f = CossFunction.LSM() # 计算损失函数得值 coss = loss_f.coss(fz, input_y)[0][0] # 求损失函数得导数 dz = loss_f.derivative(fz, input_y) # 计算w的导数 dW = np.matmul(np.transpose(input_x), dz) / (2 * n_sample) # 计算b的导数 db = dz / (2 * n_sample) # 反向更新 W和b的值 self.W = self.W - self.leaning_rate * dW self.b = self.b - self.leaning_rate * db[0] return coss def predict(self, input_x): return np.matmul(input_x, self.W) + self.bclass LSM: """ 最小乘二法 """ def coss(self,input_z,input_y): assert len(input_z.shape) == len(input_y.shape) return np.matmul(np.transpose(input_z-input_y),(input_z-input_y)) def derivative(self,input_z,input_y): """ 计算导数 :param input_z: :param input_y: :return: """ return 2.0*(input_z-input_y) 运行测试: 1234567891011121314151617181920212223242526272829303132import numpy as npfrom sklearn import datasetsfrom LinearModel import LinearRegressionimport matplotlib.pyplot as plt#加载数据集irisdata = datasets.load_iris()# 我们这里只使用一个特征，方便画散点图。xdata = irisdata.data[:,2]xdata= np.reshape(xdata,newshape=(-1,1))ydata = irisdata.data[:,3]ydata= np.reshape(ydata,newshape=(-1,1))clf = LinearRegression(leaning_rate=0.1)plt.figure(1)plt.ion()plt.xlabel("x")plt.ylabel("y")plt.grid(True)plt.title("iris dataset")plt.scatter(xdata,ydata , marker="o", c="r")line=Noneplt.show()for i in range(10000000): coss=clf.fit(xdata,ydata) #画函数图 if line is None: line, = plt.plot(xdata, clf.predict(xdata), color='g') line.set_ydata(clf.predict(xdata)) plt.pause(0.01) print("coss:", coss) 代码参考github 运行效果：]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
</search>
